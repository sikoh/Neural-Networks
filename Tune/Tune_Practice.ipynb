{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS_423_Tune_Assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "0.22.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGGrt9EYlCqY"
      },
      "source": [
        "\n",
        "\n",
        "# Hyperparameter Tuning Practice\n",
        "\n",
        "\n",
        "# Gridsearch Hyperparameters\n",
        "\n",
        "In the guided project, you learned how to use sklearn's `GridsearchCV` and `keras-tuner` libraries to tune the hyperparameters of a neural network model. For your module project you'll continue using these two libraries, however we are going to make things a little more interesting for you.\n",
        "\n",
        "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class.\n",
        "\n",
        "\n",
        "\n",
        "**Don't forget to switch to GPU on Colab!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7oEgGCV3_hY"
      },
      "source": [
        "## 0.1 Imports and installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxctNMPb7mNY"
      },
      "source": [
        "# native python libraries imports\n",
        "import math\n",
        "from time import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sklearn imports\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# keras imports\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.activations import relu, sigmoid\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "# required for compatibility between sklearn and keras\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# install keras-tuner\n",
        "!pip install keras-tuner\n",
        "from kerastuner.tuners import RandomSearch, BayesianOptimization, Sklearn\n",
        "from kerastuner.engine.hyperparameters import HyperParameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMBS8CRBzYqB"
      },
      "source": [
        "## 0.2 Load quickdraw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr8w6IX37mNa"
      },
      "source": [
        "def load_quickdraw10():\n",
        "    \"\"\"\n",
        "    Fill out this doc string, and comment the code, for practice in writing the kind of code that will get you hired.\n",
        "    \"\"\"\n",
        "\n",
        "    URL_ = \"https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/quickdraw10.npz?raw=true\"\n",
        "\n",
        "    path_to_zip = get_file('./quickdraw10.npz', origin=URL_, extract=False)\n",
        "\n",
        "    data = np.load(path_to_zip)\n",
        "\n",
        "    # normalize your image data\n",
        "    max_pixel_value = 255\n",
        "    X = data['arr_0']/max_pixel_value\n",
        "    Y = data['arr_1']\n",
        "\n",
        "    return train_test_split(X, Y, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjU5nY3e7mNc"
      },
      "source": [
        "X_train, X_test, y_train, y_test = load_quickdraw10()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkvBPoUy7mNd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89bbc747-b3f4-4f65-f901-0fcacbf79a21"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4dx6VA07mNe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "332d91fe-a58b-466f-ca69-9c6d9e9673e0"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75000,)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXsWtj8Z7mNf"
      },
      "source": [
        "_____\n",
        "\n",
        "# Experiment 1\n",
        "\n",
        "## Tune Hyperperameters using Enhanced GridsearchCV\n",
        "\n",
        "We are going to use GridsearchCV again to tune a deep learning model however we are going to add some additional functionality to our gridsearch.\n",
        "\n",
        "Specifically, we are going to automate the generation of how many nodes to use in a layer and how many layers to use in a model!\n",
        "\n",
        "By the way, yes, there is a function within a function. Try to not let that bother you. An alternative to this would be to create a class. If you're up for the challenge give it a shot. However, consider this a stretch goal that you come back to after you finish going through this assignment.\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "The objective of this experiment is to show you how to automate the generation of layers and layer nodes for the purposes of gridsearch. <br>\n",
        "Up until now, we've been manually selecting the number of layers and layer nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USXjs7Hk71Hy"
      },
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(n_layers,  first_layer_nodes, last_layer_nodes, act_funct =\"relu\", negative_node_incrementation=True):\n",
        "    \"\"\"\"\n",
        "    Returns a compiled keras model\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_layers: int\n",
        "        number of hidden layers in model\n",
        "        To be clear, this excludes the input and output layer.\n",
        "\n",
        "    first_layer_nodes: int\n",
        "        Number of nodes in the first hidden layer\n",
        "\n",
        "    last_layer_nodes: int\n",
        "        Number of nodes in the last hidden layer (this is the layer just prior to the output layer)\n",
        "\n",
        "     act_funct: string\n",
        "         Name of activation function to use in hidden layers (this excludes the output layer)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model: keras object\n",
        "    \"\"\"\n",
        "\n",
        "    def gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation=True):\n",
        "        \"\"\"\n",
        "        Generates and returns the number of nodes in each hidden layer.\n",
        "        To be clear, this excludes the input and output layer.\n",
        "\n",
        "        Note\n",
        "        ----\n",
        "        Number of nodes in each layer is linearly incremented.\n",
        "        For example, gen_layer_nodes(5, 500, 100) will generate [500, 400, 300, 200, 100]\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_layers: int\n",
        "            Number of hidden layers\n",
        "            This values should be 2 or greater\n",
        "\n",
        "        first_layer_nodes: int\n",
        "\n",
        "        last_layer_nodes: int\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        layers: list of ints\n",
        "            Contains number of nodes for each layer\n",
        "        \"\"\"\n",
        "\n",
        "        # throws an error if n_layers is less than 2\n",
        "        assert n_layers >= 2, \"n_layers must be 2 or greater\"\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # PROTIP: IF YOU WANT THE NODE INCREMENTATION TO BE SPACED DIFFERENTLY\n",
        "        # THEN YOU'LL NEED TO CHANGE THE WAY THAT IT'S CALCULATED - HAVE FUN!\n",
        "        # when set to True number of nodes are decreased for subsequent layers\n",
        "        # NOTE: the order of the number of nodes doesn't matter\n",
        "        if negative_node_incrementation:\n",
        "            # subtract this amount from previous layer's nodes in order to increment towards smaller numbers\n",
        "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
        "\n",
        "        # when set to False number of nodes are increased for subsequent layers\n",
        "        else:\n",
        "            # add this amount from previous layer's nodes in order to increment towards larger numbers\n",
        "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
        "\n",
        "        nodes = first_layer_nodes\n",
        "\n",
        "        for i in range(1, n_layers+1):\n",
        "\n",
        "            layers.append(math.ceil(nodes))\n",
        "\n",
        "            # increment nodes for next layer\n",
        "            nodes = nodes + nodes_increment\n",
        "\n",
        "        return layers\n",
        "\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "\n",
        "    n_nodes = gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation)\n",
        "\n",
        "    for i in range(1, n_layers):\n",
        "        if i==1:\n",
        "            model.add(Dense(first_layer_nodes, input_dim=X_train.shape[1], activation=act_funct))\n",
        "        else:\n",
        "            model.add(Dense(n_nodes[i-1], activation=act_funct))\n",
        "\n",
        "\n",
        "    # output layer\n",
        "    model.add(Dense(10, # 10 unit/neurons in output layer because we have 10 possible labels to predict\n",
        "                    activation='softmax')) # use softmax for a label set greater than 2\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer='adam', # adam is a good default optimizer\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # do not include model.fit() inside the create_model function\n",
        "    # KerasClassifier is expecting a complied model\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YO-x0nqt7mNh"
      },
      "source": [
        "## 1.1 Explore `create_model`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The helper function `gen_layer_nodes()` which is contained inside `create_model()` <br>\n",
        "returns a list containing the number of nodes for each successive layer.<br>\n",
        "\n",
        "Let's check that `gen_layer_nodes()` behaves as expected. <br>\n",
        "In other words, we'll perform a **Unit Test!**"
      ],
      "metadata": {
        "id": "-1hnjQHKW19w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation=True):\n",
        "        \"\"\"\n",
        "        Generates and returns the number of nodes in each hidden layer.\n",
        "        To be clear, this excludes the input and output layer.\n",
        "\n",
        "        Note\n",
        "        ----\n",
        "        Number of nodes in each layer is linearly incremented.\n",
        "        For example, gen_layer_nodes(5, 500, 100) will generate [500, 400, 300, 200, 100]\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_layers: int\n",
        "            Number of hidden layers\n",
        "            This values should be 2 or greater\n",
        "\n",
        "        first_layer_nodes: int\n",
        "\n",
        "        last_layer_nodes: int\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        layers: list of ints\n",
        "            Contains number of nodes for each layer\n",
        "        \"\"\"\n",
        "\n",
        "        # throws an error if n_layers is less than 2\n",
        "        assert n_layers >= 2, \"n_layers must be 2 or greater\"\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # PROTIP: IF YOU WANT THE NODE INCREMENTATION TO BE SPACED DIFFERENTLY\n",
        "        # THEN YOU'LL NEED TO CHANGE THE WAY THAT IT'S CALCULATED - HAVE FUN!\n",
        "        # when set to True number of nodes are decreased for subsequent layers\n",
        "        # NOTE: the order of the number of nodes doesn't matter\n",
        "        if negative_node_incrementation:\n",
        "            # subtract this amount from previous layer's nodes in order to increment towards smaller numbers\n",
        "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
        "            #print(f'nodes increment = {nodes_increment}')\n",
        "\n",
        "        # when set to False number of nodes are increased for subsequent layers\n",
        "        else:\n",
        "            # add this amount from previous layer's nodes in order to increment towards larger numbers\n",
        "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
        "            #print(f'nodes increment = {nodes_increment}')\n",
        "\n",
        "        nodes = first_layer_nodes\n",
        "\n",
        "        for i in range(1, n_layers+1):\n",
        "\n",
        "            layers.append(math.ceil(nodes))\n",
        "\n",
        "            # increment nodes for next layer\n",
        "            nodes = nodes + nodes_increment\n",
        "\n",
        "        return layers\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YiPXu0p_Qco_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `negative_node_incrementation = True`\n",
        "For this case we want the number of nodes to _decrease_ by a constant number for successive layers. <br>So `first_layer_nodes` must be _larger_ than `last_layer_nodes`"
      ],
      "metadata": {
        "id": "Mj3MrB6jXUMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_layers = 5\n",
        "first_layer_nodes = 500\n",
        "last_layer_nodes = 100\n",
        "negative_node_incrementation = True\n",
        "n_nodes = gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation)\n",
        "print(f'Number of nodes in successive layers: {n_nodes}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m4jRNllXPPG",
        "outputId": "e01496fd-55c7-49a8-9033-f0d9ae74a06a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes in successive layers: [500, 400, 300, 200, 100]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `negative_node_incrementation = False`\n",
        "For this case we want the number of nodes to _increase_ by a constant number for successive layers. <br>So `first_layer_nodes` must be _smaller_ than `last_layer_nodes`"
      ],
      "metadata": {
        "id": "ttkaf3g9XhGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_layers = 5\n",
        "first_layer_nodes = 100\n",
        "last_layer_nodes = 500\n",
        "negative_node_incrementation = False\n",
        "n_nodes = gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation)\n",
        "print(f'Number of nodes in successive layers: {n_nodes}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fkrMS8bXQUo",
        "outputId": "8c35a81d-6fa8-41c3-ba55-e87c13ed521b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes in successive layers: [100, 200, 300, 400, 500]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OK, the Unit Test is passed!"
      ],
      "metadata": {
        "id": "FHuB-bm5Wkpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's build a few models<br>\n",
        "in order to understand how `create_model()` works in practice."
      ],
      "metadata": {
        "id": "qO3AjVWOZ6SA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95E85Ug07mNh"
      },
      "source": [
        "### Build a model, setting `negative_node_incrementation = True`\n",
        "\n",
        "Use `create_model` to build a model.\n",
        "\n",
        "- Set `n_layers = 10`\n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5dcf5c585f07629a03086cf57ba53615",
          "grade": false,
          "grade_id": "cell-86d63e89a21223de",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "x_1REOCY7mNi"
      },
      "source": [
        "# use create_model to create a model\n",
        "\n",
        "# YOUR CODE HERE\n",
        "model = create_model(n_layers = 10, first_layer_nodes = 500, last_layer_nodes = 100, act_funct = \"relu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYMwZQ7k7mNi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e21fc6d-7d90-41f0-d31c-c0f4af94b7f2"
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built\n",
        "# Notice in the model summary how the number of nodes have been linearly incremented in decreasing values.\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 500)               392500    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 456)               228456    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 412)               188284    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 367)               151571    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 323)               118864    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 278)               90072     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 234)               65286     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 189)               44415     \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 145)               27550     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 10)                1460      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,308,458\n",
            "Trainable params: 1,308,458\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUc0jfnRm-uh"
      },
      "source": [
        "### Build a model, setting `negative_node_incrementation = False`\n",
        "\n",
        "Use `create_model` to build a model.\n",
        "\n",
        "- Set `n_layers = 10`\n",
        "- Set `first_layer_nodes = 100`\n",
        "- Set `last_layer_nodes = 500`\n",
        "- Set `act_funct = \"relu\"`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5dcf5c585f07629a03086cf57ba53615",
          "grade": false,
          "grade_id": "cell-86d63e89a21223de",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "3_-kqHQtm-ui"
      },
      "source": [
        "# use create_model to create a model\n",
        "\n",
        "# YOUR CODE HERE\n",
        "model = create_model(n_layers = 10, first_layer_nodes = 100, last_layer_nodes = 500, act_funct = \"relu\", negative_node_incrementation=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piboKWsNm-uj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f241dd27-7f7c-476b-9d75-4f76a878868c"
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built\n",
        "# Notice in the model summary how the number of nodes have been linearly incremented in decreasing values.\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_10 (Dense)            (None, 100)               78500     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 145)               14645     \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 189)               27594     \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 234)               44460     \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 278)               65330     \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 323)               90117     \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 367)               118908    \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 412)               151616    \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 456)               188328    \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 10)                4570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 784,068\n",
            "Trainable params: 784,068\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBH7AR9p0OXi"
      },
      "source": [
        "## 1.2 Create a grid search using `sklearn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veloj7Nnlttf"
      },
      "source": [
        "### Hyperparameter search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e2lhZqP7mNn"
      },
      "source": [
        "# define the grid search parameters\n",
        "param_grid = {'n_layers': [2, 3],\n",
        "              'epochs': [3],\n",
        "              \"first_layer_nodes\": [500, 300],\n",
        "              \"last_layer_nodes\": [100, 50]\n",
        "             }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ks_MLPB7mNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd3cb598-01e0-4092-ce88-348470737500"
      },
      "source": [
        "model = KerasClassifier(create_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8GKbLJ_7mNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02743bff-2422-4595-a26f-fe820cfd6b71"
      },
      "source": [
        "%%time\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model,\n",
        "                    param_grid=param_grid,\n",
        "                    n_jobs=-2,\n",
        "                    verbose=1,\n",
        "                    cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6520 - accuracy: 0.8008\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.4485 - accuracy: 0.8621\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3657 - accuracy: 0.8871\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4576 - accuracy: 0.8646\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6620 - accuracy: 0.7977\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4510 - accuracy: 0.8633\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3665 - accuracy: 0.8875\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4722 - accuracy: 0.8584\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6503 - accuracy: 0.8021\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.4436 - accuracy: 0.8652\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3586 - accuracy: 0.8895\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4746 - accuracy: 0.8608\n",
            "Epoch 1/3\n",
            "2344/2344 [==============================] - 6s 3ms/step - loss: 0.5898 - accuracy: 0.8195\n",
            "Epoch 2/3\n",
            "2344/2344 [==============================] - 6s 3ms/step - loss: 0.4106 - accuracy: 0.8743\n",
            "Epoch 3/3\n",
            "2344/2344 [==============================] - 6s 3ms/step - loss: 0.3339 - accuracy: 0.8976\n",
            "Best: 0.8660399913787842 using {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8645200133323669, Stdev: 0.0021654431503067106 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.8660399913787842, Stdev: 0.002128926919270058 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8647866646448771, Stdev: 0.0008580995367371738 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8643333315849304, Stdev: 0.001356981713059123 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 3}\n",
            "Means: 0.8617066542307535, Stdev: 0.0004759012921167335 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.8626933296521505, Stdev: 0.002732582737414984 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8613199989000956, Stdev: 0.0013821862857215867 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8612800041834513, Stdev: 0.002582608952283948 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 3}\n",
            "CPU times: user 7min 10s, sys: 37.5 s, total: 7min 48s\n",
            "Wall time: 7min 59s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfH6okqe7mNo"
      },
      "source": [
        "best_model = grid_result.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Inlda_0w7mNo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b91b858-5bc1-4ab7-99c4-515a917fe3ce"
      },
      "source": [
        "best_model.get_params()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'build_fn': <function __main__.create_model>,\n",
              " 'epochs': 3,\n",
              " 'first_layer_nodes': 500,\n",
              " 'last_layer_nodes': 100,\n",
              " 'n_layers': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrs3Yib17mNl"
      },
      "source": [
        "Ok, now that we've played around a bit with  `create_model`, let's build a  simpler model that we'll use to run gridsearches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvegpS1-5yYX"
      },
      "source": [
        "### Build model\n",
        "\n",
        "Use `create_model` to build a model.\n",
        "\n",
        "- Set `n_layers = 2`\n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n",
        "- Make sure that `negative_node_incrementation = True`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-4ca6c5e51302fd10",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "p-NcKYRr5yYX"
      },
      "source": [
        "# use create_model to create a model\n",
        "\n",
        "###BEGIN SOLUTION\n",
        "model = create_model(2,500,100,'relu')\n",
        "\n",
        "###END SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICLd6cYN5yYY",
        "outputId": "df23c68c-0f49-41ad-e7f9-d5ec29c74fb2"
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_83 (Dense)            (None, 500)               392500    \n",
            "                                                                 \n",
            " dense_84 (Dense)            (None, 10)                5010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 397,510\n",
            "Trainable params: 397,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwY6GFo85yYY"
      },
      "source": [
        "# define the grid search parameters\n",
        "param_grid = {'n_layers': [2, 3],\n",
        "              'epochs': [3],\n",
        "              \"first_layer_nodes\": [500, 300],\n",
        "              \"last_layer_nodes\": [100, 50]\n",
        "             }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a0iHBqJ5yYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b058f3da-193f-45d1-e18b-47682a988754"
      },
      "source": [
        "model = KerasClassifier(create_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxpuM3g15yYZ",
        "outputId": "7be6ce8e-cb1f-483e-9441-fbd0b8edffea"
      },
      "source": [
        "%%time\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model,\n",
        "                    param_grid=param_grid,\n",
        "                    n_jobs=-2,\n",
        "                    verbose=1,\n",
        "                    cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6675 - accuracy: 0.8007\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.4535 - accuracy: 0.8646\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.3615 - accuracy: 0.8911\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4732 - accuracy: 0.8646\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6664 - accuracy: 0.8004\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.4493 - accuracy: 0.8657\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.3621 - accuracy: 0.8920\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4651 - accuracy: 0.8646\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6624 - accuracy: 0.8013\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.4489 - accuracy: 0.8655\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.3596 - accuracy: 0.8912\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4832 - accuracy: 0.8579\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6352 - accuracy: 0.8043\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4364 - accuracy: 0.8663\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3489 - accuracy: 0.8921\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4604 - accuracy: 0.8671\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6348 - accuracy: 0.8081\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4292 - accuracy: 0.8690\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3437 - accuracy: 0.8944\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4508 - accuracy: 0.8675\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6315 - accuracy: 0.8063\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4279 - accuracy: 0.8695\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.3409 - accuracy: 0.8958\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4662 - accuracy: 0.8637\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6627 - accuracy: 0.8023\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.4493 - accuracy: 0.8649\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.3624 - accuracy: 0.8895\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4909 - accuracy: 0.8571\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6672 - accuracy: 0.7998\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.4542 - accuracy: 0.8657\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.3654 - accuracy: 0.8901\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4605 - accuracy: 0.8675\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6662 - accuracy: 0.7986\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.4513 - accuracy: 0.8660\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.3630 - accuracy: 0.8919\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4663 - accuracy: 0.8651\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6353 - accuracy: 0.8052\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4305 - accuracy: 0.8676\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3479 - accuracy: 0.8918\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4630 - accuracy: 0.8645\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6371 - accuracy: 0.8064\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.4315 - accuracy: 0.8671\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3426 - accuracy: 0.8953\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4684 - accuracy: 0.8620\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6387 - accuracy: 0.8039\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4329 - accuracy: 0.8678\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3430 - accuracy: 0.8947\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4670 - accuracy: 0.8619\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6913 - accuracy: 0.7935\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.4742 - accuracy: 0.8595\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.3892 - accuracy: 0.8839\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4919 - accuracy: 0.8586\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6945 - accuracy: 0.7925\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.4762 - accuracy: 0.8584\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.3940 - accuracy: 0.8818\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4764 - accuracy: 0.8587\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6955 - accuracy: 0.7912\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.4732 - accuracy: 0.8585\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.3869 - accuracy: 0.8858\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4801 - accuracy: 0.8621\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6612 - accuracy: 0.7967\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4500 - accuracy: 0.8630\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3649 - accuracy: 0.8872\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4642 - accuracy: 0.8628\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6590 - accuracy: 0.7987\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4493 - accuracy: 0.8619\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3627 - accuracy: 0.8893\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4659 - accuracy: 0.8630\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6522 - accuracy: 0.8011\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4448 - accuracy: 0.8645\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3619 - accuracy: 0.8898\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4778 - accuracy: 0.8611\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6843 - accuracy: 0.7953\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4738 - accuracy: 0.8591\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3903 - accuracy: 0.8832\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4810 - accuracy: 0.8592\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6878 - accuracy: 0.7945\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4749 - accuracy: 0.8591\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3888 - accuracy: 0.8853\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4734 - accuracy: 0.8641\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6880 - accuracy: 0.7952\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4711 - accuracy: 0.8601\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3879 - accuracy: 0.8846\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4865 - accuracy: 0.8578\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6603 - accuracy: 0.7991\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4495 - accuracy: 0.8637\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3664 - accuracy: 0.8868\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4980 - accuracy: 0.8580\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6628 - accuracy: 0.7994\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4511 - accuracy: 0.8619\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3650 - accuracy: 0.8874\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4615 - accuracy: 0.8630\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6562 - accuracy: 0.7984\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4480 - accuracy: 0.8634\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3639 - accuracy: 0.8887\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4851 - accuracy: 0.8565\n",
            "Epoch 1/3\n",
            "2344/2344 [==============================] - 7s 3ms/step - loss: 0.5920 - accuracy: 0.8197\n",
            "Epoch 2/3\n",
            "2344/2344 [==============================] - 6s 3ms/step - loss: 0.4104 - accuracy: 0.8748\n",
            "Epoch 3/3\n",
            "2344/2344 [==============================] - 6s 3ms/step - loss: 0.3370 - accuracy: 0.8973\n",
            "Best: 0.8661066691080729 using {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8623466690381368, Stdev: 0.003158456786498566 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.8661066691080729, Stdev: 0.0017236614490344477 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8632266521453857, Stdev: 0.004451856710974633 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8627866705258688, Stdev: 0.0012260934007349723 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 3}\n",
            "Means: 0.8598000009854635, Stdev: 0.0016129311624541085 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.8622933228810629, Stdev: 0.0008322414013593413 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.860373338063558, Stdev: 0.0027102446816245906 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8591599861780802, Stdev: 0.002778192618729175 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 3}\n",
            "CPU times: user 7min 13s, sys: 37.3 s, total: 7min 51s\n",
            "Wall time: 8min 27s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIlpwjag5yYZ"
      },
      "source": [
        "best_model = grid_result.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFvMxmr85yYZ",
        "outputId": "680b0076-9585-4471-c3e2-406781f58f50"
      },
      "source": [
        "best_model.get_params()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'build_fn': <function __main__.create_model>,\n",
              " 'epochs': 3,\n",
              " 'first_layer_nodes': 500,\n",
              " 'last_layer_nodes': 100,\n",
              " 'n_layers': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6azV65Nb7mNo"
      },
      "source": [
        "-----\n",
        "\n",
        "# Experiment 2: Run the Gridsearch Algorithms\n",
        "\n",
        "In this section, we are going to use the same model and dataset in order to benchmark 3 different gridsearch approaches:\n",
        "\n",
        "- Gridsearch\n",
        "- Random Search\n",
        "- Bayesian Optimization.\n",
        "\n",
        "\n",
        "Our goal in this experiment is two-fold. We want to see which appraoch\n",
        "\n",
        "- Scores the highest accuracy\n",
        "- Has the shortest run time\n",
        "\n",
        "We want to see how these 3 gridsearch approaches handle these trade-offs and to give you a sense of those trades offs.\n",
        "\n",
        "### Trade-offs\n",
        "\n",
        "`Gridsearch` will train a model on every single unique hyperparameter combination, this guarantees that you'll get the highest possible accuracy from your parameter set but your gridsearch might have a very long run-time.\n",
        "\n",
        "`Random Search` will randomly sample from your parameter set which, depending on how many samples, the run-time might be significantly cut down but you might or might not sample the parameters that correspond to the heightest possible accuracies.\n",
        "\n",
        "`Bayesian Optimization` has a bit of intelligence built into it's search algorithm but you do need to manually select some parameters which may greatly influence the model learning outcomes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X41u_hls7mNp"
      },
      "source": [
        "-------\n",
        "### Build our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ_uyKlj7mNp"
      },
      "source": [
        "# because gridsearching can take a lot of time and we are bench marking 3 different approaches\n",
        "# let's build a simple model to minimize run time\n",
        "\n",
        "def build_model(hp):\n",
        "\n",
        "    \"\"\"\n",
        "    Returns a complied keras model ready for keras-tuner gridsearch algorithms\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # hidden layer\n",
        "    model.add(Dense(units=hp.get('units'),activation=hp.get(\"activation\")))\n",
        "\n",
        "    # output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(hp.get('learning_rate')),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYE7rTku7mNp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5b7a9b44-143c-4f95-e215-3bac10db7f2f"
      },
      "source": [
        "# build out our hyperparameter dictionary\n",
        "hp = HyperParameters()\n",
        "hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "hp.Choice('learning_rate',values=[1e-1, 1e-2, 1e-3])\n",
        "hp.Choice('activation',values=[\"relu\", \"sigmoid\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'relu'"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqjp2kHD7mNu"
      },
      "source": [
        "---------\n",
        "## 2.1 Gridsearch Optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsNW4rJp7mNu"
      },
      "source": [
        "### Populate a `sklearn` compatible parameter dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJQFKoyL7mNu"
      },
      "source": [
        "# build out our hyperparameter dictionary\n",
        "hyper_parameters = {\n",
        "    # BUG Fix: cast array as list otherwise GridSearchCV will throw error\n",
        "    \"units\": np.arange(32, 512, 32).tolist(),\n",
        "    \"learning_rate\": [1e-1, 1e-2, 1e-3],\n",
        "    \"activation\":[\"relu\", \"sigmoid\"]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcxV58iC7mNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce6cf5ff-0830-43b9-f677-af4546b52efd"
      },
      "source": [
        "hyper_parameters"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': ['relu', 'sigmoid'],\n",
              " 'learning_rate': [0.1, 0.01, 0.001],\n",
              " 'units': [32,\n",
              "  64,\n",
              "  96,\n",
              "  128,\n",
              "  160,\n",
              "  192,\n",
              "  224,\n",
              "  256,\n",
              "  288,\n",
              "  320,\n",
              "  352,\n",
              "  384,\n",
              "  416,\n",
              "  448,\n",
              "  480]}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOoMg9Ao7mNv"
      },
      "source": [
        "### Build a `sklearn` compatible model function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZFVl-I-7mNv"
      },
      "source": [
        "def build_model2(units, learning_rate, activation):\n",
        "\n",
        "    \"\"\"\n",
        "    Returns a complie keras model ready for keras-tuner gridsearch algorithms\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # hidden layer\n",
        "    model.add(Dense(units, activation=activation))\n",
        "\n",
        "    # output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqYmn3QFsqZ_"
      },
      "source": [
        "### Apply the \"wrapper\" to make the model compatible with `sklearn`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABSzrTrH7mNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "079740f1-f055-4b44-c5ef-b7fcc470c218"
      },
      "source": [
        "model = KerasClassifier(build_fn = build_model2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "tTawllrN7mNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4963870-07be-4cf4-dc2b-97558f594d42"
      },
      "source": [
        "# save start time\n",
        "start = time()\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model,\n",
        "                    param_grid=hyper_parameters,\n",
        "                    n_jobs=-2,\n",
        "                    verbose=1,\n",
        "                    cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# save end time\n",
        "end = time()\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 90 candidates, totalling 270 fits\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.8448 - accuracy: 0.3302\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9402 - accuracy: 0.3302\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9633 - accuracy: 0.2793\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0505 - accuracy: 0.2080\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.1808 - accuracy: 0.1827\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1702 - accuracy: 0.1677\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9360 - accuracy: 0.3022\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9141 - accuracy: 0.3243\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0885 - accuracy: 0.2205\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0597 - accuracy: 0.1940\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0594 - accuracy: 0.2434\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1400 - accuracy: 0.1848\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9603 - accuracy: 0.3001\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9251 - accuracy: 0.2730\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9124 - accuracy: 0.3132\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8693 - accuracy: 0.3080\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9207 - accuracy: 0.3309\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9814 - accuracy: 0.2448\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9723 - accuracy: 0.3176\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9857 - accuracy: 0.2643\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9354 - accuracy: 0.3251\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8626 - accuracy: 0.3559\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9483 - accuracy: 0.3221\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8107 - accuracy: 0.3263\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.8487 - accuracy: 0.4166\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8998 - accuracy: 0.3198\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9292 - accuracy: 0.3453\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9836 - accuracy: 0.2488\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9470 - accuracy: 0.3262\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9590 - accuracy: 0.2738\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.1495 - accuracy: 0.2468\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1337 - accuracy: 0.2235\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9685 - accuracy: 0.3288\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9672 - accuracy: 0.2646\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9639 - accuracy: 0.3231\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8275 - accuracy: 0.3102\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9617 - accuracy: 0.3687\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8802 - accuracy: 0.3120\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0665 - accuracy: 0.2969\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9363 - accuracy: 0.2880\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9596 - accuracy: 0.3273\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8969 - accuracy: 0.3024\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9377 - accuracy: 0.3717\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0367 - accuracy: 0.2479\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0770 - accuracy: 0.2508\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0484 - accuracy: 0.2226\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9231 - accuracy: 0.3570\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9510 - accuracy: 0.2898\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0897 - accuracy: 0.2978\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0168 - accuracy: 0.2251\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0000 - accuracy: 0.3172\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1070 - accuracy: 0.2448\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0730 - accuracy: 0.3086\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9400 - accuracy: 0.2428\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0641 - accuracy: 0.3264\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0213 - accuracy: 0.2310\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9975 - accuracy: 0.3324\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9714 - accuracy: 0.3394\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9819 - accuracy: 0.3622\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9329 - accuracy: 0.3465\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0584 - accuracy: 0.3395\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8855 - accuracy: 0.3217\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0825 - accuracy: 0.3336\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0484 - accuracy: 0.3090\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.2565 - accuracy: 0.2281\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0454 - accuracy: 0.2303\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0982 - accuracy: 0.3423\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9743 - accuracy: 0.2645\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0325 - accuracy: 0.3977\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.3304 - accuracy: 0.2870\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0855 - accuracy: 0.3688\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9834 - accuracy: 0.2364\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0905 - accuracy: 0.3331\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0571 - accuracy: 0.2957\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.1833 - accuracy: 0.2694\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0780 - accuracy: 0.2343\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.1085 - accuracy: 0.3006\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1794 - accuracy: 0.1460\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.1181 - accuracy: 0.3195\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9018 - accuracy: 0.2774\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.3333 - accuracy: 0.2370\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0966 - accuracy: 0.2001\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.2372 - accuracy: 0.2948\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1335 - accuracy: 0.2249\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.2013 - accuracy: 0.2922\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0255 - accuracy: 0.2797\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0484 - accuracy: 0.3624\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7942 - accuracy: 0.3713\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.1625 - accuracy: 0.3690\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8932 - accuracy: 0.3091\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8037 - accuracy: 0.7540\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7084 - accuracy: 0.7920\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8130 - accuracy: 0.7535\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7317 - accuracy: 0.7755\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7726 - accuracy: 0.7652\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6754 - accuracy: 0.7967\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7646 - accuracy: 0.7673\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6960 - accuracy: 0.7951\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7611 - accuracy: 0.7682\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7106 - accuracy: 0.7874\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7502 - accuracy: 0.7726\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6887 - accuracy: 0.7950\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7485 - accuracy: 0.7723\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6683 - accuracy: 0.8032\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7530 - accuracy: 0.7699\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6640 - accuracy: 0.8031\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7534 - accuracy: 0.7691\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6726 - accuracy: 0.7982\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7402 - accuracy: 0.7771\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6437 - accuracy: 0.8114\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7480 - accuracy: 0.7746\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6657 - accuracy: 0.8050\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7393 - accuracy: 0.7760\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6400 - accuracy: 0.8146\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7412 - accuracy: 0.7746\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6698 - accuracy: 0.7970\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7430 - accuracy: 0.7769\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6545 - accuracy: 0.8039\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7459 - accuracy: 0.7743\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7190 - accuracy: 0.7785\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7423 - accuracy: 0.7744\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7178 - accuracy: 0.7977\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7477 - accuracy: 0.7739\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6715 - accuracy: 0.8038\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7442 - accuracy: 0.7756\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6767 - accuracy: 0.8018\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7406 - accuracy: 0.7736\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6867 - accuracy: 0.7967\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7371 - accuracy: 0.7789\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6316 - accuracy: 0.8090\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7463 - accuracy: 0.7738\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6946 - accuracy: 0.7890\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7360 - accuracy: 0.7782\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6561 - accuracy: 0.8090\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7398 - accuracy: 0.7779\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6559 - accuracy: 0.8026\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7366 - accuracy: 0.7769\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6529 - accuracy: 0.8056\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7362 - accuracy: 0.7770\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6759 - accuracy: 0.7976\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7454 - accuracy: 0.7742\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6571 - accuracy: 0.8064\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7344 - accuracy: 0.7776\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7069 - accuracy: 0.7912\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7431 - accuracy: 0.7744\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6561 - accuracy: 0.8065\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7481 - accuracy: 0.7744\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6772 - accuracy: 0.8044\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7409 - accuracy: 0.7772\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6345 - accuracy: 0.8139\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7399 - accuracy: 0.7772\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7060 - accuracy: 0.8009\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7427 - accuracy: 0.7772\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6445 - accuracy: 0.8108\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7443 - accuracy: 0.7768\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6603 - accuracy: 0.8060\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7418 - accuracy: 0.7776\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6562 - accuracy: 0.8064\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7540 - accuracy: 0.7740\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6572 - accuracy: 0.8010\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7462 - accuracy: 0.7753\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6572 - accuracy: 0.8051\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7469 - accuracy: 0.7768\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6813 - accuracy: 0.8010\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7562 - accuracy: 0.7715\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6481 - accuracy: 0.8066\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7413 - accuracy: 0.7779\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6806 - accuracy: 0.7960\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7484 - accuracy: 0.7749\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6708 - accuracy: 0.8067\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7477 - accuracy: 0.7737\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6371 - accuracy: 0.8115\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7447 - accuracy: 0.7767\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7142 - accuracy: 0.7885\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7387 - accuracy: 0.7759\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6559 - accuracy: 0.8091\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7546 - accuracy: 0.7749\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7033 - accuracy: 0.7910\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7356 - accuracy: 0.7803\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6696 - accuracy: 0.8039\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8806 - accuracy: 0.7392\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7403 - accuracy: 0.7846\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8583 - accuracy: 0.7435\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7218 - accuracy: 0.7872\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8681 - accuracy: 0.7393\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7157 - accuracy: 0.7904\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8004 - accuracy: 0.7608\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6525 - accuracy: 0.8080\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8103 - accuracy: 0.7588\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6434 - accuracy: 0.8111\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8041 - accuracy: 0.7637\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6576 - accuracy: 0.8098\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7726 - accuracy: 0.7698\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6351 - accuracy: 0.8140\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7787 - accuracy: 0.7689\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6104 - accuracy: 0.8197\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7661 - accuracy: 0.7724\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6159 - accuracy: 0.8178\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7432 - accuracy: 0.7792\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6075 - accuracy: 0.8192\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7455 - accuracy: 0.7790\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5899 - accuracy: 0.8272\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7419 - accuracy: 0.7792\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5996 - accuracy: 0.8234\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7310 - accuracy: 0.7820\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5852 - accuracy: 0.8277\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7255 - accuracy: 0.7842\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5723 - accuracy: 0.8315\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7273 - accuracy: 0.7819\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5826 - accuracy: 0.8283\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7228 - accuracy: 0.7841\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5758 - accuracy: 0.8311\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7185 - accuracy: 0.7876\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5765 - accuracy: 0.8287\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7176 - accuracy: 0.7863\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5742 - accuracy: 0.8314\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7073 - accuracy: 0.7877\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5711 - accuracy: 0.8304\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7097 - accuracy: 0.7884\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5640 - accuracy: 0.8318\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7058 - accuracy: 0.7890\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5827 - accuracy: 0.8311\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6954 - accuracy: 0.7917\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5678 - accuracy: 0.8326\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6990 - accuracy: 0.7900\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5586 - accuracy: 0.8369\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6935 - accuracy: 0.7958\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5540 - accuracy: 0.8365\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6911 - accuracy: 0.7937\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5558 - accuracy: 0.8369\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6895 - accuracy: 0.7938\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5576 - accuracy: 0.8334\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6855 - accuracy: 0.7960\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5485 - accuracy: 0.8381\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6812 - accuracy: 0.7956\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5408 - accuracy: 0.8394\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6891 - accuracy: 0.7946\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5454 - accuracy: 0.8376\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6840 - accuracy: 0.7954\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5406 - accuracy: 0.8379\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6791 - accuracy: 0.7974\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5400 - accuracy: 0.8418\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6822 - accuracy: 0.7974\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5387 - accuracy: 0.8442\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6850 - accuracy: 0.7952\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5539 - accuracy: 0.8394\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6782 - accuracy: 0.7971\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5451 - accuracy: 0.8415\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6814 - accuracy: 0.7974\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5399 - accuracy: 0.8380\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6770 - accuracy: 0.7972\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5398 - accuracy: 0.8420\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6736 - accuracy: 0.7974\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5330 - accuracy: 0.8427\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6744 - accuracy: 0.7985\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5403 - accuracy: 0.8378\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6665 - accuracy: 0.8004\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5377 - accuracy: 0.8416\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6653 - accuracy: 0.8001\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5484 - accuracy: 0.8347\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6740 - accuracy: 0.7992\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5346 - accuracy: 0.8394\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6675 - accuracy: 0.8005\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5468 - accuracy: 0.8372\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6654 - accuracy: 0.8013\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5342 - accuracy: 0.8410\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6705 - accuracy: 0.7985\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5260 - accuracy: 0.8435\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6650 - accuracy: 0.8007\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5340 - accuracy: 0.8417\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1594 - accuracy: 0.6270\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.0460 - accuracy: 0.6822\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.2169 - accuracy: 0.6039\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1297 - accuracy: 0.6411\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.2207 - accuracy: 0.6027\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1295 - accuracy: 0.6608\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1441 - accuracy: 0.6379\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.0298 - accuracy: 0.6846\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1668 - accuracy: 0.6314\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2508 - accuracy: 0.5912\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1026 - accuracy: 0.6429\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.9971 - accuracy: 0.6822\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1999 - accuracy: 0.6272\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1420 - accuracy: 0.6541\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1676 - accuracy: 0.6388\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.9690 - accuracy: 0.7071\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1420 - accuracy: 0.6432\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.0788 - accuracy: 0.6596\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.2053 - accuracy: 0.6249\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1398 - accuracy: 0.6561\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.2069 - accuracy: 0.6303\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1677 - accuracy: 0.6695\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.2141 - accuracy: 0.6216\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1647 - accuracy: 0.6414\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.2677 - accuracy: 0.6165\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1927 - accuracy: 0.6618\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.2420 - accuracy: 0.6269\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3295 - accuracy: 0.6285\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.3108 - accuracy: 0.6130\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3436 - accuracy: 0.6158\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.1669 - accuracy: 0.6447\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1273 - accuracy: 0.6502\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.2265 - accuracy: 0.6230\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1978 - accuracy: 0.6665\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.3510 - accuracy: 0.6145\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1188 - accuracy: 0.6758\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.3640 - accuracy: 0.6091\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.4373 - accuracy: 0.5878\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.2509 - accuracy: 0.6343\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1179 - accuracy: 0.6682\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.2634 - accuracy: 0.6183\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2248 - accuracy: 0.6548\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.2484 - accuracy: 0.6318\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1382 - accuracy: 0.6548\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.4705 - accuracy: 0.6030\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7039 - accuracy: 0.5813\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.4140 - accuracy: 0.5964\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.5545 - accuracy: 0.5738\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.4581 - accuracy: 0.6057\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3984 - accuracy: 0.6188\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4530 - accuracy: 0.6091\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2384 - accuracy: 0.6605\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.3652 - accuracy: 0.6177\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1910 - accuracy: 0.6638\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.4282 - accuracy: 0.6045\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.4379 - accuracy: 0.6235\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.4545 - accuracy: 0.5998\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7833 - accuracy: 0.5807\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.6230 - accuracy: 0.5767\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3716 - accuracy: 0.6494\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.6779 - accuracy: 0.5813\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.4289 - accuracy: 0.6445\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.5623 - accuracy: 0.5928\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.0902 - accuracy: 0.7096\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.1935 - accuracy: 0.6373\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1781 - accuracy: 0.6438\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.8347 - accuracy: 0.5635\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0573 - accuracy: 0.5475\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.7183 - accuracy: 0.5800\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.3137 - accuracy: 0.6520\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.6128 - accuracy: 0.5924\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7231 - accuracy: 0.5452\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.6126 - accuracy: 0.5826\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.3707 - accuracy: 0.5541\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.4020 - accuracy: 0.6115\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1616 - accuracy: 0.6645\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4841 - accuracy: 0.5955\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1475 - accuracy: 0.6778\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.3853 - accuracy: 0.6136\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2343 - accuracy: 0.6367\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.8442 - accuracy: 0.5757\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.3532 - accuracy: 0.5588\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.2740 - accuracy: 0.6210\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2658 - accuracy: 0.6228\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.2287 - accuracy: 0.6402\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.0859 - accuracy: 0.6726\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4726 - accuracy: 0.5976\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2424 - accuracy: 0.6367\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.5050 - accuracy: 0.6058\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2139 - accuracy: 0.6814\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8009 - accuracy: 0.7540\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7100 - accuracy: 0.7883\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7888 - accuracy: 0.7609\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7081 - accuracy: 0.7810\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7882 - accuracy: 0.7601\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.7909\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7299 - accuracy: 0.7763\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6505 - accuracy: 0.8047\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7312 - accuracy: 0.7767\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6207 - accuracy: 0.8109\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7284 - accuracy: 0.7763\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6515 - accuracy: 0.8070\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7142 - accuracy: 0.7793\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6470 - accuracy: 0.8046\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7143 - accuracy: 0.7811\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6287 - accuracy: 0.8087\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7086 - accuracy: 0.7839\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6204 - accuracy: 0.8126\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7008 - accuracy: 0.7858\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6064 - accuracy: 0.8187\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6961 - accuracy: 0.7873\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6070 - accuracy: 0.8158\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7014 - accuracy: 0.7848\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6197 - accuracy: 0.8158\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6897 - accuracy: 0.7861\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5932 - accuracy: 0.8214\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6970 - accuracy: 0.7841\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6185 - accuracy: 0.8123\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6930 - accuracy: 0.7866\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5944 - accuracy: 0.8188\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6951 - accuracy: 0.7860\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6155 - accuracy: 0.8117\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6934 - accuracy: 0.7877\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5879 - accuracy: 0.8241\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6913 - accuracy: 0.7895\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6291 - accuracy: 0.8093\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6934 - accuracy: 0.7859\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6062 - accuracy: 0.8204\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6886 - accuracy: 0.7880\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6007 - accuracy: 0.8174\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6897 - accuracy: 0.7871\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6398 - accuracy: 0.8069\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6903 - accuracy: 0.7855\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5924 - accuracy: 0.8227\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6917 - accuracy: 0.7893\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6118 - accuracy: 0.8110\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6922 - accuracy: 0.7869\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6027 - accuracy: 0.8152\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6905 - accuracy: 0.7878\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5985 - accuracy: 0.8180\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6905 - accuracy: 0.7878\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5793 - accuracy: 0.8235\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6815 - accuracy: 0.7900\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6148 - accuracy: 0.8160\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6898 - accuracy: 0.7871\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5943 - accuracy: 0.8210\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6961 - accuracy: 0.7856\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6267 - accuracy: 0.8018\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6882 - accuracy: 0.7887\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6070 - accuracy: 0.8188\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6897 - accuracy: 0.7889\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6100 - accuracy: 0.8175\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6925 - accuracy: 0.7856\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5986 - accuracy: 0.8181\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6923 - accuracy: 0.7878\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6021 - accuracy: 0.8105\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6853 - accuracy: 0.7899\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5929 - accuracy: 0.8184\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6938 - accuracy: 0.7868\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6262 - accuracy: 0.8075\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6879 - accuracy: 0.7881\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6030 - accuracy: 0.8212\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6866 - accuracy: 0.7880\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5923 - accuracy: 0.8209\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6914 - accuracy: 0.7868\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6202 - accuracy: 0.8096\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6978 - accuracy: 0.7851\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6146 - accuracy: 0.8134\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6859 - accuracy: 0.7887\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6151 - accuracy: 0.8178\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6994 - accuracy: 0.7862\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5853 - accuracy: 0.8197\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6906 - accuracy: 0.7880\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5994 - accuracy: 0.8146\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6905 - accuracy: 0.7881\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6599 - accuracy: 0.7932\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6928 - accuracy: 0.7870\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5800 - accuracy: 0.8248\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6904 - accuracy: 0.7878\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5822 - accuracy: 0.8273\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.0313 - accuracy: 0.7072\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.8110 - accuracy: 0.7622\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.0449 - accuracy: 0.7045\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.8101 - accuracy: 0.7608\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.0226 - accuracy: 0.7068\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.8133 - accuracy: 0.7595\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.9383 - accuracy: 0.7224\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7614 - accuracy: 0.7726\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.9357 - accuracy: 0.7263\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7504 - accuracy: 0.7811\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.9363 - accuracy: 0.7295\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7524 - accuracy: 0.7761\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8912 - accuracy: 0.7380\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7250 - accuracy: 0.7886\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8899 - accuracy: 0.7407\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7177 - accuracy: 0.7899\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8899 - accuracy: 0.7393\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7249 - accuracy: 0.7836\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8728 - accuracy: 0.7430\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7052 - accuracy: 0.7920\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8779 - accuracy: 0.7415\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7119 - accuracy: 0.7935\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8692 - accuracy: 0.7457\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7091 - accuracy: 0.7915\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8516 - accuracy: 0.7505\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7003 - accuracy: 0.7938\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8595 - accuracy: 0.7451\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6998 - accuracy: 0.7941\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8573 - accuracy: 0.7491\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7027 - accuracy: 0.7889\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8426 - accuracy: 0.7497\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6960 - accuracy: 0.7950\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8487 - accuracy: 0.7497\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6875 - accuracy: 0.7981\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8447 - accuracy: 0.7507\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6916 - accuracy: 0.7983\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8387 - accuracy: 0.7501\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6957 - accuracy: 0.7976\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8362 - accuracy: 0.7529\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6882 - accuracy: 0.7979\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8384 - accuracy: 0.7520\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.7011 - accuracy: 0.7914\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8297 - accuracy: 0.7539\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.7995\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8370 - accuracy: 0.7513\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6896 - accuracy: 0.7976\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8357 - accuracy: 0.7515\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6909 - accuracy: 0.7954\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8289 - accuracy: 0.7520\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6880 - accuracy: 0.7992\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8287 - accuracy: 0.7523\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6753 - accuracy: 0.8040\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8242 - accuracy: 0.7563\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6906 - accuracy: 0.7910\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8282 - accuracy: 0.7513\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6825 - accuracy: 0.8007\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8257 - accuracy: 0.7536\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6763 - accuracy: 0.8000\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8219 - accuracy: 0.7565\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6778 - accuracy: 0.8022\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8187 - accuracy: 0.7551\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6727 - accuracy: 0.8045\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8256 - accuracy: 0.7523\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6664 - accuracy: 0.8029\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8295 - accuracy: 0.7532\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6834 - accuracy: 0.7982\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8230 - accuracy: 0.7539\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6746 - accuracy: 0.8014\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8180 - accuracy: 0.7579\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6712 - accuracy: 0.8027\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8241 - accuracy: 0.7544\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6767 - accuracy: 0.8008\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8170 - accuracy: 0.7543\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6719 - accuracy: 0.8064\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8262 - accuracy: 0.7548\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6921 - accuracy: 0.7878\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8235 - accuracy: 0.7551\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6873 - accuracy: 0.7977\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8157 - accuracy: 0.7575\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6808 - accuracy: 0.8017\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8169 - accuracy: 0.7552\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6551 - accuracy: 0.8101\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.8168 - accuracy: 0.7561\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6747 - accuracy: 0.8012\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8177 - accuracy: 0.7542\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6769 - accuracy: 0.8002\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8229 - accuracy: 0.7544\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6733 - accuracy: 0.8003\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8154 - accuracy: 0.7564\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6725 - accuracy: 0.8000\n",
            "2344/2344 [==============================] - 7s 3ms/step - loss: 0.6153 - accuracy: 0.8160\n",
            "Best: 0.8420666654904684 using {'activation': 'relu', 'learning_rate': 0.001, 'units': 480}\n",
            "Means: 0.23532000184059143, Stdev: 0.06911144917026836 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 32}\n",
            "Means: 0.23434666295846304, Stdev: 0.06373151728237604 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 64}\n",
            "Means: 0.2752666672070821, Stdev: 0.025818432251109694 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 96}\n",
            "Means: 0.31550665696461994, Stdev: 0.03815324233405269 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 128}\n",
            "Means: 0.28082666794459027, Stdev: 0.02940840067168555 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 160}\n",
            "Means: 0.2660933385292689, Stdev: 0.03543621205106485 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 192}\n",
            "Means: 0.30084000031153363, Stdev: 0.009863056930725695 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 224}\n",
            "Means: 0.2534400075674057, Stdev: 0.02774289968705461 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 256}\n",
            "Means: 0.23758666714032492, Stdev: 0.008851510829330105 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 288}\n",
            "Means: 0.30565333366394043, Stdev: 0.052839604559413715 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 320}\n",
            "Means: 0.28699999550978345, Stdev: 0.04044187922126789 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 352}\n",
            "Means: 0.26259999970595044, Stdev: 0.02070009609727211 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 384}\n",
            "Means: 0.22533333798249564, Stdev: 0.06146861018212773 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 416}\n",
            "Means: 0.23411999642848969, Stdev: 0.03220286988050036 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 448}\n",
            "Means: 0.3200266758600871, Stdev: 0.038172248981083276 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 480}\n",
            "Means: 0.7880666851997375, Stdev: 0.009082159949934778 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 32}\n",
            "Means: 0.792520006497701, Stdev: 0.00359214276227254 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 64}\n",
            "Means: 0.8014933466911316, Stdev: 0.002328797060518107 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 96}\n",
            "Means: 0.8103466629981995, Stdev: 0.004026743474375021 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 128}\n",
            "Means: 0.7931466499964396, Stdev: 0.010721503642130393 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 160}\n",
            "Means: 0.8011200030644735, Stdev: 0.002565811781744716 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 192}\n",
            "Means: 0.7982266743977865, Stdev: 0.008205515534712842 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 224}\n",
            "Means: 0.8057733376820883, Stdev: 0.002614485668825361 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 256}\n",
            "Means: 0.7984266678492228, Stdev: 0.006246524800779095 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 288}\n",
            "Means: 0.8082533280054728, Stdev: 0.0040752271392513 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 320}\n",
            "Means: 0.8058800101280212, Stdev: 0.004050218417214987 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 352}\n",
            "Means: 0.8041733304659525, Stdev: 0.002335194722679704 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 384}\n",
            "Means: 0.8012000123659769, Stdev: 0.004362393076014822 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 416}\n",
            "Means: 0.802239994208018, Stdev: 0.009925164099080988 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 448}\n",
            "Means: 0.8013466596603394, Stdev: 0.00761795971272165 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 480}\n",
            "Means: 0.7874133586883545, Stdev: 0.0024036260490277925 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 32}\n",
            "Means: 0.8096533417701721, Stdev: 0.0012805344138583363 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 64}\n",
            "Means: 0.8171866536140442, Stdev: 0.0023590439550428156 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 96}\n",
            "Means: 0.8232933282852173, Stdev: 0.0032668641730098412 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 128}\n",
            "Means: 0.8291600147883097, Stdev: 0.0016611744773463526 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 160}\n",
            "Means: 0.8303733269373575, Stdev: 0.0012028204736402664 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 192}\n",
            "Means: 0.8311199943224589, Stdev: 0.0005722436654250172 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 224}\n",
            "Means: 0.8353333473205566, Stdev: 0.001938350332483201 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 256}\n",
            "Means: 0.836133340994517, Stdev: 0.0020212988613270864 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 288}\n",
            "Means: 0.8383066654205322, Stdev: 0.0008147463627322391 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 320}\n",
            "Means: 0.8417733311653137, Stdev: 0.0019432957904031156 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 352}\n",
            "Means: 0.8404800097147623, Stdev: 0.0017645465898981983 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 384}\n",
            "Means: 0.840719997882843, Stdev: 0.002111281514835148 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 416}\n",
            "Means: 0.8370933334032694, Stdev: 0.0019283959552186987 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 448}\n",
            "Means: 0.8420666654904684, Stdev: 0.001073464539143387 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 480}\n",
            "Means: 0.6614000002543131, Stdev: 0.016791836744983177 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 32}\n",
            "Means: 0.6526533365249634, Stdev: 0.0434926696967834 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 64}\n",
            "Means: 0.6736133495966593, Stdev: 0.023771557374125285 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 96}\n",
            "Means: 0.6556533376375834, Stdev: 0.011467600264425006 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 128}\n",
            "Means: 0.6353733539581299, Stdev: 0.019394609401296115 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 160}\n",
            "Means: 0.6641600131988525, Stdev: 0.010579129675206373 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 192}\n",
            "Means: 0.6369600097338358, Stdev: 0.035189185147369344 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 224}\n",
            "Means: 0.6033200025558472, Stdev: 0.03655926217156581 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 256}\n",
            "Means: 0.6476933360099792, Stdev: 0.020446299166046747 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 288}\n",
            "Means: 0.6178666750590006, Stdev: 0.028338186520154605 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 320}\n",
            "Means: 0.6659733255704244, Stdev: 0.030849803738665065 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 352}\n",
            "Means: 0.5815866589546204, Stdev: 0.04982643107592984 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 384}\n",
            "Means: 0.6321333448092142, Stdev: 0.05542794273253988 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 416}\n",
            "Means: 0.6061066587766012, Stdev: 0.03389953903089857 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 448}\n",
            "Means: 0.6635466615358988, Stdev: 0.019306516852829456 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 480}\n",
            "Means: 0.786733349164327, Stdev: 0.00418662462299215 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 32}\n",
            "Means: 0.807533323764801, Stdev: 0.0025429385504434805 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 64}\n",
            "Means: 0.8086400032043457, Stdev: 0.0032991333751885236 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 96}\n",
            "Means: 0.8167600035667419, Stdev: 0.0013576342842329186 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 128}\n",
            "Means: 0.8175333340962728, Stdev: 0.0038361549690261857 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 160}\n",
            "Means: 0.8150399923324585, Stdev: 0.006497340207708799 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 192}\n",
            "Means: 0.8149199883143107, Stdev: 0.005791394765294542 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 224}\n",
            "Means: 0.8163066506385803, Stdev: 0.0048367886718220085 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 256}\n",
            "Means: 0.8191466728846232, Stdev: 0.0031752991746295586 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 288}\n",
            "Means: 0.8138533234596252, Stdev: 0.00860176828578051 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 320}\n",
            "Means: 0.8153466780980428, Stdev: 0.0034499682310024614 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 352}\n",
            "Means: 0.8157066702842712, Stdev: 0.005925270443395761 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 384}\n",
            "Means: 0.8146133224169413, Stdev: 0.004700336043463458 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 416}\n",
            "Means: 0.8173466523488363, Stdev: 0.0021146599985708706 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 448}\n",
            "Means: 0.8150666753451029, Stdev: 0.015524468311638248 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 480}\n",
            "Means: 0.7608133355776469, Stdev: 0.0010784385479848302 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 32}\n",
            "Means: 0.7766133348147074, Stdev: 0.0034632358583911757 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 64}\n",
            "Means: 0.7873466412226359, Stdev: 0.0027345338623567853 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 96}\n",
            "Means: 0.792306661605835, Stdev: 0.0008524882939334961 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 128}\n",
            "Means: 0.7922666668891907, Stdev: 0.0023692208902314337 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 160}\n",
            "Means: 0.7971066832542419, Stdev: 0.001520112573000424 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 192}\n",
            "Means: 0.7956399917602539, Stdev: 0.0029734701179645615 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 224}\n",
            "Means: 0.7975333531697592, Stdev: 0.0016673590372529952 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 256}\n",
            "Means: 0.7980800072352091, Stdev: 0.005337971998601991 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 288}\n",
            "Means: 0.800986667474111, Stdev: 0.0009017600665755909 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 320}\n",
            "Means: 0.8018666704495748, Stdev: 0.002646323626368516 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 352}\n",
            "Means: 0.8016266822814941, Stdev: 0.0007744808551351538 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 384}\n",
            "Means: 0.7973333398501078, Stdev: 0.007598333925336618 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 416}\n",
            "Means: 0.8043199976285299, Stdev: 0.004079343285453693 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 448}\n",
            "Means: 0.8001599907875061, Stdev: 0.00011774125777296196 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 480}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THKMZLNv7mNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a415f10-c61a-4569-be6c-2e761604be3e"
      },
      "source": [
        "# total run time\n",
        "total_run_time_in_miniutes = (end - start)/60\n",
        "total_run_time_in_miniutes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33.65523448387782"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XgJsrZb7mNx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcf290db-6c19-4fe4-9e93-773f6946554d"
      },
      "source": [
        "grid_result.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'relu', 'learning_rate': 0.001, 'units': 480}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYufbSI87mNx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2fda0e9-df9d-467e-f5e5-1ca2600e2e90"
      },
      "source": [
        "# because all other optimization approaches are reporting test set score\n",
        "# let's calculate the test set score in this case\n",
        "best_model = grid_result.best_estimator_\n",
        "test_acc = best_model.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4762 - accuracy: 0.8634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj4jJ0Qm7mNx"
      },
      "source": [
        " ### Results\n",
        "\n",
        "Identify and write the the best performing hyperparameter combination and model score.\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9577db883482c6cded3836e5cfbf5a74",
          "grade": true,
          "grade_id": "cell-eb06d682d2790f6e",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "10px3N2q7mNx"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu-lBWph7mNq"
      },
      "source": [
        "------\n",
        "## 2.2 Random Search with `keras-tuner`\n",
        "\n",
        "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `RandomSearch` tuner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "aaff9aae33845f374e15f2381719d83a",
          "grade": false,
          "grade_id": "cell-8c1dfb9b6d12bea2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "8DApqLli7mNq"
      },
      "source": [
        "# how many unique hyperparameter combinations do we have?\n",
        "# HINT: take the product of the number of possible values for each hyperparameter\n",
        "# save your answer to n_unique_hparam_combos\n",
        "\n",
        "# YOUR CODE HERE\n",
        "n_unique_hparam_combos = len(np.arange(32, 512, 32).tolist()) * len([1e-1, 1e-2, 1e-3]) * len([\"relu\", \"sigmoid\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a9d628451e83431e1b52da10eccf2c00",
          "grade": false,
          "grade_id": "cell-1fa83950bb2d5f92",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "m1UKRA597mNq"
      },
      "source": [
        "# how many of these do we want to randomly sample?\n",
        "# let's pick 25% of n_unique_hparam_combos param combos to sample\n",
        "# save this number to n_param_combos_to_sample\n",
        "\n",
        "# YOUR CODE HERE\n",
        "fraction_to_sample = 0.25\n",
        "n_param_combos_to_sample = n_unique_hparam_combos * fraction_to_sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TzaNnzoQU4U"
      },
      "source": [
        "### Instantiate a `RandomSearch()` object for your grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9PCHLBWQPcb"
      },
      "source": [
        "random_tuner = RandomSearch(\n",
        "            build_model,\n",
        "            objective='val_accuracy',\n",
        "            max_trials=n_param_combos_to_sample, # number of times to sample the parameter set and build a model\n",
        "            seed=1234,\n",
        "            hyperparameters=hp, # pass in our hyperparameter dictionary\n",
        "            directory='./keras-tuner-trial',\n",
        "            project_name='random_search')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGFdv1qE7mNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2399b22d-88ba-426e-8afe-2d617f6e10e0"
      },
      "source": [
        "# take note of Total elapsed time in print out -- took ~10 minutes without GPU\n",
        "random_tuner.search(X_train, y_train,\n",
        "                    epochs=3,\n",
        "                    validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 23 Complete [00h 00m 41s]\n",
            "val_accuracy: 0.8608400225639343\n",
            "\n",
            "Best val_accuracy So Far: 0.8742799758911133\n",
            "Total elapsed time: 00h 13m 23s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNBUhIe97mNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a12fb52d-eec7-4b72-a2d0-cfb8482ade48"
      },
      "source": [
        "# identify the best score and hyperparamter (should be at the top since scores are ranked)\n",
        "random_tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in ./keras-tuner-trial/random_search\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 384\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8730000257492065\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 352\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8729199767112732\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 448\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8636400103569031\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 480\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8621199727058411\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 416\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8591200113296509\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 288\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8572400212287903\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 224\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8525599837303162\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 320\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8378400206565857\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 160\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8370400071144104\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 192\n",
            "learning_rate: 0.01\n",
            "activation: sigmoid\n",
            "Score: 0.8350800275802612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "FRpQVXBE7mNr"
      },
      "source": [
        " ### Results\n",
        "\n",
        "Identify and write the the best performing hyperparameter combination and model score.\n",
        "Note that because this is Random Search, multiple runs might have slighly different outcomes.\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f084b5d373f8589a1de8d6d4473b974a",
          "grade": true,
          "grade_id": "cell-5527738b6382c164",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "aQjMc84c7mNs"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXjW7eYA7mNs"
      },
      "source": [
        "------\n",
        "## 2.3 Bayesian Optimization with `keras-tuner`\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/0/02/GpParBayesAnimationSmall.gif)\n",
        "\n",
        "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `BayesianOptimization` tuner.\n",
        "\n",
        "Pay special attention to these `BayesianOptimization` parameters: `num_initial_points` and `beta`.\n",
        "\n",
        "`num_initial_points`:\n",
        "\n",
        "Number of randomly selected hyperparameter combinations to try before applying bayesian probability to determine liklihood of which param combo to try next based on expected improvement\n",
        "\n",
        "\n",
        "`beta`:\n",
        "\n",
        "Larger values means more willing to explore new hyperparameter combinations (analogous to searching for the global minimum in Gradient Descent), smaller values means that it is less willing to try new hyperparameter combinations (analogous to getting stuck in a local minimum in Gradient Descent).\n",
        "\n",
        "As a start, error on the side of larger values. What defines a small or large value you ask? That question would pull us into the mathematical intricacies of Bayesian Optimization and Gaussian Processes. For simplicity, notice that the default value is 2.6 and work from there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NXjQBn47mNs"
      },
      "source": [
        "# we know that 24 samples is about 25% of 96 possible hyper-parameter combos\n",
        "# let's set up a run with the same parameters we used for RandomSearch() so the comparison will be aplles-to-apples\n",
        "# feel free to play with any of these numbers later\n",
        "max_trials=24\n",
        "num_initial_points=5\n",
        "beta=5.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhZNIJZ4RS5Y"
      },
      "source": [
        "#### Instantiate a `BayesianOptimization()` object for your grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33joO_J97mNs"
      },
      "source": [
        "bayesian_tuner = BayesianOptimization(\n",
        "                    build_model,\n",
        "                    objective='val_accuracy',\n",
        "                    max_trials=max_trials,\n",
        "                    hyperparameters=hp, # pass in our hyperparameter dictionary\n",
        "                    num_initial_points=num_initial_points,\n",
        "                    beta=beta,\n",
        "                    seed=1234,\n",
        "                    directory='./keras-tuner-trial',\n",
        "                    project_name='bayesian_optimization_4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9AM5Pdj7mNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6fbd15c-f894-4a1a-b3ae-d763435c1aa5"
      },
      "source": [
        "bayesian_tuner.search(X_train, y_train,\n",
        "               epochs=3,\n",
        "               validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 24 Complete [00h 00m 41s]\n",
            "val_accuracy: 0.8284000158309937\n",
            "\n",
            "Best val_accuracy So Far: 0.8763999938964844\n",
            "Total elapsed time: 00h 14m 25s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "FJcHC8d87mNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1adc2a63-d230-49bc-f949-8783571d1102"
      },
      "source": [
        "bayesian_tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in ./keras-tuner-trial/bayesian_optimization_4\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 512\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8763999938964844\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 352\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8736400008201599\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 480\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8673200011253357\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 192\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8591600060462952\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 256\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.858519971370697\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 192\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8571199774742126\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 192\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8552799820899963\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 192\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8551599979400635\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 192\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8550800085067749\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 192\n",
            "learning_rate: 0.001\n",
            "activation: sigmoid\n",
            "Score: 0.8541600108146667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woo9D9AU7mNu"
      },
      "source": [
        " ### Results\n",
        "\n",
        "Identify and write the the best performing hyperparameter combination and model score.\n",
        "Note that because this is  Bayesian Optimization, multiple runs might have slighly different outcomes.\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1badcdca408cdd49bc2e409dca3bac5a",
          "grade": true,
          "grade_id": "cell-ff95600bf745f40f",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "1EXa47mH7mNu"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOZ5-tJDraFE"
      },
      "source": [
        "We should point out that Gridsearch split the training set internally and created a test set whereas keras-tuner allows us to pass in a test set. This means that the keras-tuner algorithms were using one test set and our skearn GridSearchCV was using a different test set - so this isn't a perfectly exact 1-to-1 comparision but it'll have to do. In order to compensate for this, we did score the best model on the same test set that keras-tuner used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPYChhrC7mNx"
      },
      "source": [
        "_______\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "The spirit of this experiment is to expose you to the idea of benchmarking and comparing the trade-offs of various gridsearch approaches.\n",
        "\n",
        "Even if we did find a way to pass in the original test set into GridSearchCV, we can see that both Random Search and Bayesian Optimization are arguably better alternatives to a brute force grid search when we consider the trade-offs of run time and locating the best performing model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sth1AfwX7mNy"
      },
      "source": [
        "----\n",
        "\n",
        "# Stretch Goals\n",
        "\n",
        "- Feel free to run whatever gridsearch experiments on whatever models you like!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2APQG9H7mNy"
      },
      "source": [
        "# this is your open playground - be free to explore as you wish"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}